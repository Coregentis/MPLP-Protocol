---
type: "always_apply"
description: "Testing strategy for MPLP v1.0"
---

# Testing Strategy Rules - MPLP Proven Methodology

## üèóÔ∏è **MPLP v1.0 Testing Context**

**CRITICAL**: MPLP v1.0 has **301 functional scenario tests + 66 core tests with 89.2% coverage** across 10 complete modules. All testing must maintain this production-grade quality standard.

**PROVEN METHODOLOGY**: This testing strategy has been successfully validated through complete testing of all 10 MPLP modules, discovering and fixing 12 critical source code issues.

## üéØ **Core Testing Principles**

### **Testing's Fundamental Purpose**
```markdown
RULE: Testing's fundamental purpose is to discover and fix source code problems

Core Values:
1. Discover source code issues - not bypass problems
2. Based on actual implementation - not fictional interfaces
3. Validate from user perspective - not technical-oriented
4. Fix source code - not modify test expectations
5. Ensure system stability - not local optimization

When tests reveal source code errors, immediately fix the source code
implementation, not bypass the problem. Tests should simulate production
environment and pass by fixing source code - this is testing's true value.
```

### **MPLP Testing Baseline**
```markdown
RULE: Maintain MPLP production-grade testing standards
- Maintain >89.2% test coverage
- Ensure all 367 tests continue passing (301 functional + 66 core)
- New features must include corresponding tests
- Tests must be based on actual Schema and implementation
- Functional scenario coverage must be >90%
```

## üìã **4-Layer Testing Architecture**

### **Layer 1: Functional Scenario Tests (Core Layer)**
```markdown
RULE: User scenario-based functional testing
- Target: 90%+ functional scenario coverage
- Method: Design tests from user roles and use cases
- Files: tests/functional/[module]-functional.test.ts
- Focus: Discover source code functional gaps and business logic errors

Functional Scenario Types:
‚ñ° Basic functional scenarios (most common user needs)
‚ñ° Advanced functional scenarios (professional user needs)
‚ñ° Exception handling scenarios (system robustness)
‚ñ° Boundary condition scenarios (extreme cases)
‚ñ° Integration scenarios (inter-module collaboration)
‚ñ° Performance scenarios (production environment needs)
```

### **Layer 2: Module Interface Tests (Core Layer)**
```markdown
RULE: Test module interfaces and CoreOrchestrator integration readiness
- Target: 90%+ interface coverage for CoreOrchestrator integration
- Method: Validate module interfaces with mock CoreOrchestrator data
- Files: tests/interfaces/[module]-interface.test.ts
- Focus: Ensure modules are ready for CoreOrchestrator activation

Interface Testing Methodology:
‚ñ° Test all public interfaces with CoreOrchestrator-style data
‚ñ° Validate interface signatures and return types
‚ñ° Mock CoreOrchestrator data generation patterns
‚ñ° Verify modules can handle orchestrated workflows
‚ñ° Prepare for future CoreOrchestrator integration
```

### **Layer 3: Unit Tests**
```markdown
RULE: Complete unit test coverage
- Target: 90%+ code coverage
- Method: Test individual components and functions
- Focus: Verify implementation details and boundary conditions
```

### **Layer 4: Integration Tests (CoreOrchestrator Only)**
```markdown
RULE: True integration testing only at CoreOrchestrator level
- Target: Verify CoreOrchestrator-mediated module coordination
- Method: Test through CoreOrchestrator APIs only
- Focus: Verify complete orchestrated workflows
- PROHIBITED: Direct module-to-module integration tests
- CURRENT STATUS: Deferred until CoreOrchestrator implementation
```

### **Layer 5: End-to-End Tests**
```markdown
RULE: Complete user scenario testing
- Target: Verify complete business processes
- Method: Test real user scenarios
- Focus: Verify overall system stability
```

## üîß **Chain-Based Source Code Repair Methodology**

### **Step 1: Problem Impact Analysis**
```markdown
RULE: After discovering problems, immediately analyze impact scope

Impact Analysis Checklist:
‚ñ° Direct impact: Which modules are directly affected?
‚ñ° Indirect impact: Which modules might be indirectly affected?
‚ñ° Systemic issues: Are there similar problem patterns?
‚ñ° Type definition impact: Do type definitions need updates?
‚ñ° API interface impact: Do interface definitions need updates?
‚ñ° Test impact: Which tests need corresponding adjustments?
```

### **Step 2: Systematic Repair**
```markdown
RULE: Fix source code problems, not bypass problems

Repair Principles:
‚ñ° Fix root causes, not symptoms
‚ñ° Maintain backward compatibility
‚ñ° Ensure type safety
‚ñ° Follow existing architecture patterns
‚ñ° Add necessary validation logic
‚ñ° Improve error handling mechanisms
```

### **Step 3: Chain Validation**
```markdown
RULE: After repair, must perform complete chain validation

Validation Steps:
1. TypeScript compilation validation - ensure zero compilation errors
2. Unit test validation - ensure existing functionality unaffected
3. Functional scenario test validation - ensure repair effectiveness
4. Integration test validation - ensure inter-module collaboration normal
5. End-to-end test validation - ensure overall system stability
```

## üß™ **Testing Implementation Standards**

### Functional Scenario Testing Standards
```typescript
// ‚úÖ Correct functional scenario test example
describe('Dialog Module Functional Scenarios - Based on Real User Needs', () => {
  describe('1. Dialog Creation Scenario - Dialog Administrator Daily Use', () => {
    it('should allow administrator to create a basic text dialog', async () => {
      // User scenario: Dialog administrator creates team discussion dialog
      const createRequest: CreateDialogRequest = {
        session_id: sessionId,
        context_id: contextId,
        name: 'Team Project Discussion',
        description: 'Team discussion dialog about new project',
        // ... complete data based on actual needs
      };

      const result = await dialogService.createDialog(createRequest);

      expect(result.success).toBe(true);
      expect(result.data?.name).toBe('Team Project Discussion');
      expect(result.data?.participants).toHaveLength(2);
    });
  });
});
```

## üö´ **Testing Anti-Patterns**

### Prohibited Testing Practices
```markdown
‚ùå Absolutely Prohibited:
- Modifying test expectations to fit wrong implementations
- Skipping failed test cases
- Using excessive mocks that make tests meaningless
- Testing implementation details instead of behavior
- Writing unstable tests (flaky tests)
- Dependencies between tests
- Hard-coded test data causing maintenance difficulties
- Lowering test standards to improve pass rates
- Ignoring source code problems, only focusing on test passing
```

### Correct Test Repair Methods
```markdown
‚úÖ When tests fail:
1. Analyze failure cause - is it code problem or test problem
2. If code problem - fix source code
3. If test problem - fix test logic
4. Ensure repair makes tests stable and reliable
5. Verify repair doesn't introduce new problems
6. Execute complete chain validation
7. Record problems and solutions
```

---

**Testing Commitment**: These testing strategies are based on successful MPLP project practices, ensuring enterprise-grade quality assurance capabilities supporting reliable software delivery.

**Validation Status**: ‚úÖ Successfully validated in 9 protocol modules, 654 test cases all passed, 12 source code issues fixed